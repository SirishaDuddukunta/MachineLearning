{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Random Forest Regression Explained for Beginners**\r\n\r\n#### **Concept of Random Forest Regression**\r\nImagine you're deciding whether to buy a car. Instead of relying on one person's advice, you ask 100 people. Each person gives a recommendation based on their unique experiences and observations. You then take the average of all their suggestions to make a well-rounded decision.\r\n\r\nThis is essentially how **Random Forest Regression** works:\r\n- It builds **multiple decision trees** (like asking multiple people for advice).\r\n- Each tree gives a prediction.\r\n- The final prediction is the **average** of all tree predictions, making it more **accurate** and less prone to errors than a single deceeision\n\n---e### **How Random Forest Regression is Interlinked with Other Algorithms**\n- **Decision Trees**: Random Forest builds upon Decision Trees by combining multiple trees to reduce overfitting and improve accuracy.\n- **Bagging (Bootstrap Aggregating)**: Random Forest uses bagging to create diverse trees by sampling data with replacement.\n- **Linear Models**: While linear models work best for linear relationships, Random Forest can handle complex, non-linear relationships.e.\r\n\r\n---\r\n\r\n### **Key Features**\r\n- **Ensemble Learning**: Combines multiple models (trees) for better predictions.\r\n- **Reduces Overfitting**: By averaging multiple trees, the model generalizes well.\r\n- **Handles Non-Linearity**: Captures complex patterns in data.\r\n\r\n---\r\n\r\n### **Real-World Example: Predicting House Prices**\r\n#### **Scenario**: Predict the price of a house based on features like square footage, number of bedroomsice for new house: ${predicted_[0]:, .2f}\")\r\n```\r\n\r\n---\r\n\r\n### **Output Interpretation**\r\n- **Mean Squared Error (MSE)**: Measures how far off the predictions are from the actual prices. A lower MSE indicates a better model.\r\n- **Predicted Price**: The estimated price for a new house based on learned patterns.\r\n\r\n---\r\n\r\n### **Real-World Use Cases**\r\n1. **Finance**: Predicting stock market trends based on historical data.\r\n2. **Healthcare**: Estimating patient costs or disease progression.\r\n3. **E-commerce**: Forecasting product demand and setting dynamic pricing.\r\n4. **Agriculture**: Predicting crop yields based on weather, soil qiled walkthrough for any of these exercises?","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regression:-\r\n\r\n### **Concept Overview**\r\n\r\nRandom Forest Regression is an **ensemble learning method** that uses multiple **decision trees** to predict continuous values. It overcomes the limitations of a single decision tree by averaging the predictions of multiple trees, resulting in better generalization and reduced overfitting.\r\n\r\n### **Key Concepts**\r\n1. **Ensemble Learning**: Combines multiple weak learners (decision trees) to create a strong learner.\r\n2. **Bootstrapping**: Random sampling with replacement to create diverse training subsets.\r\n3. **Feature Randomness**: Each tree uses a random subset of features to split, making trees less correlated.\r\n4. **Prediction**: Final prediction is the average of all tree predictions in regres-0 sqft, 4 bedrooms, 12 years old\r\npredicted_price = model.predict(new_house)\r\nprint(f\"Predicted Price for new house: ${predicted_price[0]:,.2f}\")  # Outpurediction\r Why Random Forest is Better Than a Single Decision Tree\nAccuracy: By averaging results, Random Forest produces more accurate predictions.\nOverfitting Reduction: Randomness in feature selection and bootstrapping reduces overfitting.\nRobustness: Works well with noisy data and complex, non-linear relationships.r relationships.\r\n\r\n---\r\n\r\n### **Practice Exercises**\r\n1. **Car Price Prediction**: Use features like mileage, year, and engine size  predict car prices.\r\n2. **E-commerce Sales Forecastinisits.\r\n3. **Weather Prediction**: Forecast temperatures using historical data (humidity, wind speed, and pressure).\r\n\r\nLet me know if you'd like further assistance with any of these exercises!","metadata":{}},{"cell_type":"code","source":"#### **Step 1: Import Necessary Libraries**\nimport numpy as np  # For handling numerical data and arrays\nfrom sklearn.ensemble import RandomForestRegressor  # Random Forest model\nfrom sklearn.model_selection import train_test_split  # Splitting dataset\nfrom sklearn.metrics import mean_squared_error, r2_score  # Model evaluation metrics\n\n#### **Step 2: Prepare Dataset**\n# Feature matrix 'X' includes square footage, number of bedrooms, and house age\n# Target variable 'y' is the corresponding house price\nX = np.array([[1500, 3, 10], [1800, 4, 5], [2400, 3, 20], [3000, 5, 15], [3500, 4, 8]])\ny = np.array([400000, 450000, 600000, 650000, 700000])\n#### **Step 3: Split Dataset**\n# Splitting data into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n#### **Step 4: Initialize the Random Forest Regressor\n# Creating a Random Forest model with 100 trees and random state for reproducibility\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n#### **Step 5: Train the Model**\n# Training the Random Forest model with the training data\nmodel.fit(X_train, y_train)\n\n#### **Step 6: Make Predictions**\n# Making predictions on the test set\npredictions = model.predict(X_test)\n\n#### **Step 7: Evaluate the Model**\n# Calculating Mean Squared Error (lower is better)\nmse = mean_squared_error(y_test, predictions)\n\n# Calculating R² Score (1.0 indicates perfect prediction)\nr2 = r2_score(y_test, predictions)\n\nprint(f\"Mean Squared Error: {mse:.2f}\")  # Displaying MSE\nprint(f\"R² Score: {r2:.2f}\")            # Displaying R² Score\n\n\n#### **Step 8: Predict for a New Data Point**\n# Predicting price for a house with specific features\nnew_house = np.array([[2500, 4, 12]])  # 2500 sqft, 4 bedrooms, 12 years old\npredicted_price = model.predict(new_house)\nprint(f\"Predicted Price for new house: ${predicted_price[0]:,.2f}\")  # Output prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T05:15:03.145612Z","iopub.execute_input":"2024-11-27T05:15:03.146062Z","iopub.status.idle":"2024-11-27T05:15:03.295892Z","shell.execute_reply.started":"2024-11-27T05:15:03.146022Z","shell.execute_reply":"2024-11-27T05:15:03.294833Z"}},"outputs":[{"name":"stdout","text":"Mean Squared Error: 4160250000.00\nR² Score: nan\nPredicted Price for new house: $605,000.00\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### **Random Forest Regression: Interview Preparation Guide**\r\n\r\n#### **Basic Concepts and Theory**\r\n\r\n1. **What is Random Forest Regression?**\r\n   - Random Forest Regression is an **ensemble learning method** that uses multiple decision trees to predict continuous values. It combines the predictions from all trees by averaging them, which improves accuracy and reduces overfitting compared to a single decision tree.\r\n\r\n2. **How does Random Forest differ from Decision Trees?**\r\n   - **Decision Tree**: A single model prone to overfitting.\r\n   - **Random Forest**: A collection of decision trees (ensemble) that reduces overfitting by averaging multiple trees' predictions.\r\n\r\n3. **Why is Random Forest called an ensemble method?**\r\n   - Because it combines the predictions of multiple models (decision trees) to produce a single robust prediction.\r\n\r\n4. **What are the main hyperparameters in Random Forest, and why are they important?**\r\n   - **`n_estimators`**: Number of trees in the forest. More trees usually improve accuracy but increase computation time.\r\n   - **`max_depth`**: Maximum depth of the trees, controlling overfitting.\r\n   - **`max_features`**: Number of features considered for splitting. Controls randomness and diversity.\r\n   - **`min_samples_split`**: Minimum number of samples needed to split a node. Prevents overfitting by creating balanced trees.\r\n   - **`min_samples_leaf`**: Minimum number of samples per leaf node, ensuring meaningful splits.\r\n\r\n---\r\n\r\n#### **Intermediate Questions**\r\n\r\n1. **How does Random Forest handle missing data?**\r\n   - Random Forest can handle missing data by:\r\n     - **Imputing missing values** using median/mode imputation or mean of non-missing values.\r\n     - **Using proximity measures** to estimate missing values by leveraging similar data points.\r\n\r\n2. **What are OOB (Out-of-Bag) errors, and how are they calculated?**\r\n   - OOB errors are calculated using data not included in the bootstrap sample (out-of-bag samples). The model predicts the output for these samples, providing a built-in validation error estimate without the need for a separate validation set.\r\n\r\n3. **What are the advantages and disadvantages of Random Forest?**\r\n   - **Advantages**:\r\n     - Handles non-linear data.\r\n     - Reduces overfitting.\r\n     - Works well with large datasets.\r\n   - **Disadvantages**:\r\n     - Requires more computational resources.\r\n     - Less interpretable compared to single decision trees.\r\n\r\n4. **How does feature importance work in Random Forest?**\r\n   - Feature importance in Random Forest is determined by calculating the average reduction in impurity (Gini or MSE) across all trees whenever a feature is used for splitting.\r\n\r\n---\r\n\r\n#### **Advanced Questions**\r\n\r\n1. **How does Random Forest handle multicollinearity?**\r\n   - Random Forest is relatively robust to multicollinearity because:\r\n     - Trees are constructed independently, reducing reliance on any single correlated feature.\r\n     - **Feature bagging** (random selection of features) further minimizes correlation effects.\r\n\r\n2. **Explain how Random Forest uses bagging and feature randomness.**\r\n   - **Bagging (Bootstrap Aggregating)**: Each tree is trained on a random sample of data with replacement.\r\n   - **Feature Randomness**: At each split, a random subset of features is considered, making the trees diverse and less prone to overfitting.\r\n\r\n3. **When would you prefer Random Forest over other regression models like Linear Regression?**\r\n   - When the data:\r\n     - Is non-linear.\r\n     - Contains complex feature interactions.\r\n     - Has missing values.\r\n     - Is prone to overfitting in simpler models.\r\n\r\n---\r\n\r\n### **Hands-On Coding Interview Questions**\r\n\r\n1. **Train a Random Forest model and evaluate its performance using Mean Squared Error (MSE).**\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.metrics import mean_squared_error\r\n\r\n# Example dataset\r\nX = [[1500, 3], [1800, 4], [2400, 3], [3000, 5], [3500, 4]]\r\ny = [400000, 450000, 600000, 650000, 700000]\r\n\r\n# Train-test split\r\nX_train = X[:-1]\r\ny_train = y[:-1]\r\nX_test = X[-1:]\r\ny_test = y[-1:]\r\n\r\n# Model training\r\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\r\nmodel.fit(X_train, y_train)\r\n\r\n# Prediction and evaluation\r\npredictions = model.predict(X_test)\r\nmse = mean_squared_error(y_test, predictions)\r\nprint(f\"Predicted Price: ${predictions[0]:,.2f}, MSE: {mse:.2f}\")\r\n```\r\n\r\n2. **Explain OOB Score in a coding example.**\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor\r\n\r\n# Initialize model with OOB scoring enabled\r\nmodel = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)\r\nmodel.fit(X_train, y_train)\r\n\r\n# Print OOB score\r\nprint(f\"OOB Score: {model.oob_score_:.2f}\")\r\n```\r\n\r\n---\r\n\r\n### **Behavioral Interview Questions**\r\n1. **Tell me about a time you used Random Forest for a project. What challenges did you face?**\r\n   - Prepare a real-world example where you applied Random Forest, discuss challenges (e.g., computational cost, overfitting), and explain how you optimized the model.\r\n\r\n2. **How would you explain Random Forest to a non-technical stakeholder?**\r\n   - Use analogies like **asking multiple experts for advice and averaging their responses** to convey the concept simply.\r\n\r\n---\r\n\r\n### **Practice Problems**\r\n\r\n1. **Predicting Sales Revenue**: Use a dataset containing product features and sales data to build a Random Forest model.\r\n2. **Predicting House Prices**: Build a model to predict house prices based on square footage, number of rooms, and location.\r\n\r\nDo you need any additional guidance or detailed solutions for these problems?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
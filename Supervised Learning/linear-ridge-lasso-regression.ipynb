{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9884919,"sourceType":"datasetVersion","datasetId":6070057}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Correct file path based on the file available in your directory\nfile_path = '/kaggle/input/supervised-learning-algorithm/updated_supervised_learning_algorithms.xlsx'\n\n# Read the Excel file (if the file contains only one sheet, or specify the sheet name if needed)\ndf = pd.read_excel(file_path, sheet_name='Sheet1')  # Specify the sheet name if needed\n\n# Set pandas to display all rows and columns\n#pd.set_option('display.max_rows', None)  # Display all rows\n#pd.set_option('display.max_columns', None)  # Display all columns\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-11-13T04:30:01.981813Z","iopub.execute_input":"2024-11-13T04:30:01.982291Z","iopub.status.idle":"2024-11-13T04:30:03.121326Z","shell.execute_reply.started":"2024-11-13T04:30:01.982248Z","shell.execute_reply":"2024-11-13T04:30:03.120046Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                       Algorithm            Type  \\\n0              Linear Regression      Regression   \n1            Logistic Regression  Classification   \n2                 Decision Trees            Both   \n3                 Random Forests            Both   \n4                            SVM            Both   \n5                            KNN            Both   \n6              Gradient Boosting            Both   \n7                    Naive Bayes  Classification   \n8          Polynomial Regression      Regression   \n9               Ridge Regression      Regression   \n10              Lasso Regression      Regression   \n11     Support Vector Regression      Regression   \n12      Decision Tree Regression      Regression   \n13      Random Forest Regression      Regression   \n14  Gradient Boosting Regression      Regression   \n\n                                              Purpose  \\\n0                    Predict continuous output values   \n1                      Predict binary output variable   \n2                        Model decisions and outcomes   \n3      Improve classification and regression accuracy   \n4   Create hyperplane for classification or predic...   \n5   Predict class or value based on k closest neig...   \n6        Combine weak learners to create strong model   \n7   Predict class based on feature independence as...   \n8   Predict continuous values with polynomial rela...   \n9   Regularized linear regression to prevent overf...   \n10           Linear regression with L1 regularization   \n11     Predict continuous values using SVM principles   \n12     Predict continuous values using decision trees   \n13  Improve prediction accuracy using multiple dec...   \n14  Ensemble method combining weak learners to imp...   \n\n                                               Method  \\\n0   Linear equation minimizing sum of squares of r...   \n1   Logistic function transforming linear relation...   \n2     Tree-like structure with decisions and outcomes   \n3                   Combining multiple decision trees   \n4   Maximizing margin between classes or predictin...   \n5   Finding k closest neighbors and predicting bas...   \n6       Iteratively correcting errors with new models   \n7   Bayes’ theorem with feature independence assum...   \n8         Polynomial terms added to linear regression   \n9         Minimizing residuals with L2 regularization   \n10        Minimizing residuals with L1 regularization   \n11          Maximizing margin between support vectors   \n12    Tree-like structure with decisions and outcomes   \n13                  Combining multiple decision trees   \n14      Iteratively correcting errors with new models   \n\n                                            Use Cases  \\\n0                        Predicting continuous values   \n1                         Binary classification tasks   \n2                 Classification and Regression tasks   \n3   Reducing overfitting, improving prediction acc...   \n4                 Classification and Regression tasks   \n5   Classification and Regression tasks, sensitive...   \n6   Classification and Regression tasks to improve...   \n7   Text classification, spam filtering, sentiment...   \n8                Predicting stock prices, forecasting   \n9         Preventing overfitting in linear regression   \n10    Feature selection, predicting continuous values   \n11            Predicting non-linear continuous values   \n12               Predicting prices, sales forecasting   \n13  Improving regression accuracy and reducing ove...   \n14  Improving prediction accuracy in regression tasks   \n\n                                       One-liner Code  \n0   from sklearn.linear_model import LinearRegress...  \n1   from sklearn.linear_model import LogisticRegre...  \n2   from sklearn.tree import DecisionTreeClassifie...  \n3   from sklearn.ensemble import RandomForestClass...  \n4   from sklearn.svm import SVC\\nmodel = SVC().fit...  \n5   from sklearn.neighbors import KNeighborsClassi...  \n6   from sklearn.ensemble import GradientBoostingC...  \n7   from sklearn.naive_bayes import GaussianNB\\nmo...  \n8   from sklearn.linear_model import PolynomialFea...  \n9   from sklearn.linear_model import Ridge\\nmodel ...  \n10  from sklearn.linear_model import Lasso\\nmodel ...  \n11  from sklearn.svm import SVR\\nmodel = SVR(kerne...  \n12  from sklearn.tree import DecisionTreeRegressor...  \n13  from sklearn.ensemble import RandomForestRegre...  \n14  from sklearn.ensemble import GradientBoostingR...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Algorithm</th>\n      <th>Type</th>\n      <th>Purpose</th>\n      <th>Method</th>\n      <th>Use Cases</th>\n      <th>One-liner Code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Linear Regression</td>\n      <td>Regression</td>\n      <td>Predict continuous output values</td>\n      <td>Linear equation minimizing sum of squares of r...</td>\n      <td>Predicting continuous values</td>\n      <td>from sklearn.linear_model import LinearRegress...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Logistic Regression</td>\n      <td>Classification</td>\n      <td>Predict binary output variable</td>\n      <td>Logistic function transforming linear relation...</td>\n      <td>Binary classification tasks</td>\n      <td>from sklearn.linear_model import LogisticRegre...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Decision Trees</td>\n      <td>Both</td>\n      <td>Model decisions and outcomes</td>\n      <td>Tree-like structure with decisions and outcomes</td>\n      <td>Classification and Regression tasks</td>\n      <td>from sklearn.tree import DecisionTreeClassifie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Random Forests</td>\n      <td>Both</td>\n      <td>Improve classification and regression accuracy</td>\n      <td>Combining multiple decision trees</td>\n      <td>Reducing overfitting, improving prediction acc...</td>\n      <td>from sklearn.ensemble import RandomForestClass...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SVM</td>\n      <td>Both</td>\n      <td>Create hyperplane for classification or predic...</td>\n      <td>Maximizing margin between classes or predictin...</td>\n      <td>Classification and Regression tasks</td>\n      <td>from sklearn.svm import SVC\\nmodel = SVC().fit...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>KNN</td>\n      <td>Both</td>\n      <td>Predict class or value based on k closest neig...</td>\n      <td>Finding k closest neighbors and predicting bas...</td>\n      <td>Classification and Regression tasks, sensitive...</td>\n      <td>from sklearn.neighbors import KNeighborsClassi...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Gradient Boosting</td>\n      <td>Both</td>\n      <td>Combine weak learners to create strong model</td>\n      <td>Iteratively correcting errors with new models</td>\n      <td>Classification and Regression tasks to improve...</td>\n      <td>from sklearn.ensemble import GradientBoostingC...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Naive Bayes</td>\n      <td>Classification</td>\n      <td>Predict class based on feature independence as...</td>\n      <td>Bayes’ theorem with feature independence assum...</td>\n      <td>Text classification, spam filtering, sentiment...</td>\n      <td>from sklearn.naive_bayes import GaussianNB\\nmo...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Polynomial Regression</td>\n      <td>Regression</td>\n      <td>Predict continuous values with polynomial rela...</td>\n      <td>Polynomial terms added to linear regression</td>\n      <td>Predicting stock prices, forecasting</td>\n      <td>from sklearn.linear_model import PolynomialFea...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Ridge Regression</td>\n      <td>Regression</td>\n      <td>Regularized linear regression to prevent overf...</td>\n      <td>Minimizing residuals with L2 regularization</td>\n      <td>Preventing overfitting in linear regression</td>\n      <td>from sklearn.linear_model import Ridge\\nmodel ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Lasso Regression</td>\n      <td>Regression</td>\n      <td>Linear regression with L1 regularization</td>\n      <td>Minimizing residuals with L1 regularization</td>\n      <td>Feature selection, predicting continuous values</td>\n      <td>from sklearn.linear_model import Lasso\\nmodel ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Support Vector Regression</td>\n      <td>Regression</td>\n      <td>Predict continuous values using SVM principles</td>\n      <td>Maximizing margin between support vectors</td>\n      <td>Predicting non-linear continuous values</td>\n      <td>from sklearn.svm import SVR\\nmodel = SVR(kerne...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Decision Tree Regression</td>\n      <td>Regression</td>\n      <td>Predict continuous values using decision trees</td>\n      <td>Tree-like structure with decisions and outcomes</td>\n      <td>Predicting prices, sales forecasting</td>\n      <td>from sklearn.tree import DecisionTreeRegressor...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Random Forest Regression</td>\n      <td>Regression</td>\n      <td>Improve prediction accuracy using multiple dec...</td>\n      <td>Combining multiple decision trees</td>\n      <td>Improving regression accuracy and reducing ove...</td>\n      <td>from sklearn.ensemble import RandomForestRegre...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Gradient Boosting Regression</td>\n      <td>Regression</td>\n      <td>Ensemble method combining weak learners to imp...</td>\n      <td>Iteratively correcting errors with new models</td>\n      <td>Improving prediction accuracy in regression tasks</td>\n      <td>from sklearn.ensemble import GradientBoostingR...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Linear Regression \nLinear regression uses the relationship between the data-points to draw a straight line through all them.\nThis line can be used to predict future values.\n\nLinear regression is one of the most fundamental algorithms in machine learning. It's used to predict a continuous target variable (or dependent variable) based on one or more input features (independent variables). The relationship between the input and the output is assumed to be linear.\n\nThe basic equation for linear regression is:\n\n𝑦 = 𝛽\n0\n+\n𝛽\n1\n𝑥\n+\n𝜖\ny=β \n0\n​\n +β \n1\n​\n x+ϵ\n \nWhere:\n\n\ny is the predicted value (dependent variable),\n\nx is the input feature (independent variable),\n𝛽\n0\nβ \n0\n​\n  is the intercept,\n𝛽\n1\nβ \n1\n​\n  is the slope (coefficient),\n𝜖\nϵ is the error term (difference between predicted and actual values).\nKey Concepts to Understand:\n\nSlope (\n𝛽\n1\nβ \n1\n​\n ): Indicates how much change in the dependent variable happens with a unit change in the independent variable.\n\nIntercept (\n𝛽\n0\nβ \n0\n​\n ): Represents the value of \n𝑦\ny \nwhen \n𝑥\n=\n0\nx=0.\n\nError (\n𝜖\nϵ): The difference between the predicted value and the actual value. We try to minimize this difference using a method called least squares.\n\nTypes of Linear Regression\n\nSimple Linear Regression: Uses one feature to predict the target variable.\n\nMultiple Linear Regression: Uses multiple features to predict the target variable.\n\nReal-World Scenarios\n\nPredicting House Prices: Based on features like square footage, number of rooms, and location.\nStock Market Prediction: Predicting future stock prices based on past data and other financial indicators.\nSales Forecasting: Estimating future sales based on advertising spend or seasonality.","metadata":{}},{"cell_type":"code","source":"#Predict the speed of a 10 years old car:\nfrom scipy import stats\n\nx = [5,7,8,7,2,17,2,9,4,11,12,9,6]\ny = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\nslope, intercept, r, p, std_err = stats.linregress(x, y)\nprint(r)\n#r for relationship and should get low value for best model\n#This relationship - the coefficient of correlation - is called r.The r value ranges from -1 to 1, \n#where 0 means no relationship, and 1 (and -1) means 100% related.\n\ndef myfunc(x):\n  return slope * x + intercept\n\nspeed = myfunc(10)\n\nprint(speed)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T16:55:13.258677Z","iopub.execute_input":"2024-11-12T16:55:13.259160Z","iopub.status.idle":"2024-11-12T16:55:13.270154Z","shell.execute_reply.started":"2024-11-12T16:55:13.259116Z","shell.execute_reply":"2024-11-12T16:55:13.269037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Dataset\nX = np.array([1500, 1800, 2400, 3000, 3500]).reshape(-1, 1)  # Square footage,.reshape(-1, 1) reshapes this array \n#to be a 2D array (with one column),This is needed because Scikit-learn expects input features in a 2D format.\n\ny = np.array([400000, 450000, 600000, 650000, 700000])  # House prices ,y is target variable\n\n# Create a Linear Regression model\nmodel = LinearRegression() #It Creates an instance of the LinearRegression model. This instance (model) will be used\n#to train on the data and make predictions.\n\n# Fit the model\nmodel.fit(X, y) #Trains (or \"fits\") the linear regression model on the dataset (X, y).\n#This step calculates the intercept and slope of the line that best fits the data points in X and y.\n\n# Print the model parameters\nprint(f\"Intercept: {model.intercept_}, Slope: {model.coef_}\")\n#model.intercept_,model.coef_:  Retrieves the INTERCEPT,SLOPE of the linear regression line\n\n# Predict house price for a new square footage (e.g., 2500 sqft)\npredicted_price = model.predict([[2500]]) #Uses the trained model to predict price of house\nprint(f\"Predicted Price for 2500 sqft house: ${predicted_price[0]:,.2f}\")\n\n# Visualize the data and the regression line\nplt.scatter(X, y, color='blue')  # Scatter plot of data- Creates a scatter plot of original data points with X,Y\nplt.plot(X, model.predict(X), color='PINK') #Plots the linear regression line using the model’s predictions on X\nplt.title(\"House Price Prediction\") #Sets the title of the plot.\nplt.xlabel(\"Square Footage\")\nplt.ylabel(\"Price\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T16:27:55.381179Z","iopub.execute_input":"2024-11-12T16:27:55.381679Z","iopub.status.idle":"2024-11-12T16:27:55.705648Z","shell.execute_reply.started":"2024-11-12T16:27:55.381632Z","shell.execute_reply":"2024-11-12T16:27:55.704566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multiple Linear Regression","metadata":{}},{"cell_type":"code","source":"# Multiple Linear Regression\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Features: Square footage, number of bedrooms, and age of the house\nX_multi = np.array([\n    [1500, 3, 10],\n    [1800, 4, 5],\n    [2400, 3, 20],\n    [3000, 4, 10],\n    [3500, 5, 15]\n])\n\n# Target: Price of the house\ny_multi = np.array([400000, 450000, 600000, 650000, 700000])\n\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_multi, y_multi)\n\n# Print model parameters\nprint(f\"Intercept: {model_multi.intercept_}\")\nprint(f\"Coefficients: {model_multi.coef_}\")\n\n# Predict price for a new house with 2500 sqft, 4 bedrooms, and 12 years old\npredicted_price_multi=model_multi.predict([[2500,4,12]])\nprint(f\"Predicted Price for 2500 sqft, 4 bedrooms, 12 years old house: ${predicted_price_multi[0]:,.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T04:14:17.605822Z","iopub.execute_input":"2024-11-13T04:14:17.606387Z","iopub.status.idle":"2024-11-13T04:14:17.619696Z","shell.execute_reply.started":"2024-11-13T04:14:17.606338Z","shell.execute_reply":"2024-11-13T04:14:17.618026Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Intercept: 248904.10958903958\nCoefficients: [   175.34246575 -33835.61643836    986.30136986]\nPredicted Price for 2500 sqft, 4 bedrooms, 12 years old house: $563,753.42\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Understanding Ridge and Lasso Regression\nBoth Ridge and Lasso regression are extensions of linear regression designed to address some of its limitations, especially overfitting in cases where there are many features or highly correlated features.\n\n## Ridge Regression\nPurpose: Ridge regression (also called L2 regularization) adds a penalty to the linear regression cost function, aiming to reduce the magnitude of the coefficients. This prevents any one feature from having too much influence on the prediction.\n\nMathematical Explanation: In regular linear regression, we minimize the sum of squared errors:\nCost = ∑ (y\n−\ny\n^\n)\n2\n\n \nIn Ridge Regression, we add a penalty term:\nCost\n=\n∑\n(\ny\n−\ny\n^\n)\n2\n+\nλ\n∑\nw\ni\n2\n\n \nwhere \nλ\nλ is a parameter that controls the strength of the penalty (regularization term), and \nw\ni\nw \ni\n​\n  are the weights (coefficients).\nEffect: With the penalty, coefficients for less important features are shrunk towards zero, but never exactly zero. This reduces model complexity and overfitting.\n\n## Lasso Regression\nPurpose: Lasso regression (short for Least Absolute Shrinkage and Selection Operator or L1 regularization) also adds a penalty to the cost function, but it penalizes the absolute values of coefficients, which can drive some coefficients to exactly zero.\nMathematical Explanation:\n\nCost=∑(y− \ny\n^\n​\n ) \n2\n +λ∑∣w \ni\n​\n ∣\n \nHere, the penalty term is the sum of absolute values of the weights.\n\nEffect: Lasso regression can reduce the coefficients of some features to zero, effectively selecting a smaller subset of features. This makes it useful for feature selection.\n\n\n### Why Ridge and Lasso Are Related to Linear Regression\nIn linear regression, we minimize the prediction error without any regularization term. This means that in cases with too many features, especially when they are highly correlated, linear regression can overfit, capturing noise rather than the true trend. Ridge and Lasso add a regularization term to the cost function, helping to prevent overfitting by controlling the model complexity.\n\n### Real-World Scenario Example\nSuppose you’re predicting the price of a house based on features like square footage, number of bedrooms, neighborhood index, etc. If there are many features and some are highly correlated (like house age and condition), linear regression might overfit by giving too much importance to some features. By applying Ridge or Lasso regression:\n\nRidge: It’ll distribute the weights more evenly, preventing any one feature from dominating.\n\nLasso: It may drop less important features, making the model simpler and more interpretable.\n\n","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Example dataset\nX = np.array([[1500, 3], [1800, 4], [2400, 3], [3000, 5], [3500, 4]])  # Square footage and number of bedrooms\ny = np.array([400000, 450000, 600000, 650000, 700000])  # Prices\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit models\nridge = Ridge(alpha=1.0) # Creates a Ridge Regression model with a regularization strength(\n#λ) of 1.0. Higher alpha means more regularization.\n\nlasso = Lasso(alpha=0.1) #Creates a Lasso Regression model with a regularization strength of \n#0.1. A lower alpha applies weaker regularization.\n\nlinear = LinearRegression() # Creates a basic Linear Regression model without regularization.\n\n# Train the models\nridge.fit(X_train, y_train)\nlasso.fit(X_train, y_train)\nlinear.fit(X_train, y_train)\n\n# Predict with each model\nridge_preds = ridge.predict(X_test)\nlasso_preds = lasso.predict(X_test)\nlinear_preds = linear.predict(X_test)\n\n# Print errors for comparison\nprint(\"Linear Regression MSE:\", mean_squared_error(y_test, linear_preds)) # Calculates the \n#Mean Squared Error(MSE),which measures the average squared diff btw actual and predicted values\nprint(\"Ridge Regression MSE:\", mean_squared_error(y_test, ridge_preds))\nprint(\"Lasso Regression MSE:\", mean_squared_error(y_test, lasso_preds))","metadata":{"execution":{"iopub.status.busy":"2024-11-20T05:37:08.894711Z","iopub.execute_input":"2024-11-20T05:37:08.895080Z","iopub.status.idle":"2024-11-20T05:37:10.986865Z","shell.execute_reply.started":"2024-11-20T05:37:08.895046Z","shell.execute_reply":"2024-11-20T05:37:10.985458Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Linear Regression MSE: 90738216.80545887\nRidge Regression MSE: 174889190.64929992\nLasso Regression MSE: 90742762.46610925\n","output_type":"stream"}]},{"cell_type":"code","source":"# Predict prices for new data points\nnew_data = np.array([[2000, 3],[2800, 4]])#Two new houses with features[square footage,no of bedrooms]\n\nridge_new_preds = ridge.predict(new_data)\nlasso_new_preds = lasso.predict(new_data)\nlinear_new_preds = linear.predict(new_data)\n\n# Display predictions\nprint(\"\\nPredictions for new data points:\")\nfor i, house in enumerate(new_data):\n    print(f\"House {i+1} (Square footage: {house[0]}, Bedrooms: {house[1]})\")\n    print(f\"  Linear Regression Predicted Price: {linear_new_preds[i]:,.2f}\")\n    print(f\"  Ridge Regression Predicted Price:  {ridge_new_preds[i]:,.2f}\")\n    print(f\"  Lasso Regression Predicted Price:  {lasso_new_preds[i]:,.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T05:41:37.078885Z","iopub.execute_input":"2024-11-20T05:41:37.079885Z","iopub.status.idle":"2024-11-20T05:41:37.089070Z","shell.execute_reply.started":"2024-11-20T05:41:37.079833Z","shell.execute_reply":"2024-11-20T05:41:37.087674Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nPredictions for new data points:\nHouse 1 (Square footage: 2000, Bedrooms: 3)\n  Linear Regression Predicted Price: 501,360.81\n  Ridge Regression Predicted Price:  500,115.02\n  Lasso Regression Predicted Price:  501,360.73\nHouse 2 (Square footage: 2800, Bedrooms: 4)\n  Linear Regression Predicted Price: 616,213.06\n  Ridge Regression Predicted Price:  616,628.33\n  Lasso Regression Predicted Price:  616,213.09\n","output_type":"stream"}]},{"cell_type":"markdown","source":" collection of **Linear, Ridge, and Lasso Regression** interview questions\n \n---\n\n### **General Linear Regression Questions**\n1. **What is Linear Regression? Explain its working.**\n   - Expected answer: A supervised learning algorithm used to predict a continuous target variable by finding a linear relationship between the input features and the target.\n\n2. **What are the assumptions of Linear Regression?**\n   - Linear relationship between features and target.\n   - Homoscedasticity (constant variance of errors).\n   - Independence of errors.\n   - No multicollinearity among features.\n   - Errors are normally distributed.\n\n3. **What is the formula for a simple Linear Regression model?**\n   - \\( y = \\beta_0 + \\beta_1x + \\epsilon \\), where:\n     - \\( y \\): Target variable.\n     - \\( \\beta_0 \\): Intercept.\n     - \\( \\beta_1 \\): Slope (coefficient for feature \\( x \\)).\n     - \\( \\epsilon \\): Error term.\n\n4. **How do you evaluate the performance of a Linear Regression model?**\n   - Metrics: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, Adjusted R-squared.\n\n5. **What are the limitations of Linear Regression?**\n   - Sensitivity to outliers.\n   - Assumes a linear relationship between features and the target.\n   - Can overfit when there are too many features without regularization.\n\n---\n\n### **Ridge Regression Questions**\n1. **What is Ridge Regression? How is it different from Linear Regression?**\n   - Ridge Regression adds an \\( L2 \\)-regularization term (\\( \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\)) to the loss function to penalize large coefficients, reducing overfitting.\n\n2. **Explain the Ridge Regression loss function.**\n   - \\( J(\\beta) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}))^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\), where \\( \\lambda \\) controls the regularization strength.\n\n3. **When should you use Ridge Regression?**\n   - When the dataset has multicollinearity or a large number of features, and you want to reduce overfitting.\n\n4. **What is the role of the hyperparameter \\( \\lambda \\) in Ridge Regression?**\n   - \\( \\lambda \\) determines the strength of regularization. Larger \\( \\lambda \\) values shrink coefficients more, potentially leading to underfitting.\n\n5. **Does Ridge Regression set any coefficients to zero?**\n   - No, Ridge Regression reduces the magnitude of coefficients but does not set them to zero.\n\n---\n\n### **Lasso Regression Questions**\n1. **What is Lasso Regression? How does it differ from Ridge Regression?**\n   - Lasso Regression adds an \\( L1 \\)-regularization term (\\( \\lambda \\sum_{j=1}^{p} |\\beta_j| \\)) to the loss function, which can shrink some coefficients to exactly zero, performing feature selection.\n\n2. **Explain the Lasso Regression loss function.**\n   - \\( J(\\beta) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}))^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\).\n\n3. **When should you use Lasso Regression?**\n   - When you want to perform feature selection or simplify a model by eliminating irrelevant features.\n\n4. **What is the significance of \\( \\lambda \\) in Lasso Regression?**\n   - \\( \\lambda \\) controls the regularization strength. Higher \\( \\lambda \\) values increase sparsity by setting more coefficients to zero.\n\n5. **What are the key differences between Lasso and Ridge Regression?**\n   - Ridge penalizes \\( L2 \\) norm; Lasso penalizes \\( L1 \\) norm.\n   - Ridge shrinks coefficients but does not set them to zero; Lasso can set coefficients to zero, performing feature selection.\n\n---\n\n### **Comparison and Practical Scenarios**\n1. **How do Ridge and Lasso Regression address multicollinearity?**\n   - Ridge: Reduces multicollinearity by penalizing large coefficients.\n   - Lasso: Eliminates irrelevant features, indirectly reducing multicollinearity.\n\n2. **What happens when \\( \\lambda = 0 \\) in Ridge or Lasso Regression?**\n   - Both regressions become equivalent to standard Linear Regression.\n\n3. **Can you combine Ridge and Lasso? What is the resulting algorithm?**\n   - Yes, the combined approach is called **Elastic Net**, which uses both \\( L1 \\) and \\( L2 \\)-regularization.\n\n4. **Which regression model would you choose for high-dimensional datasets and why?**\n   - Lasso for feature selection when many features are irrelevant.\n   - Ridge for datasets with multicollinearity or when all features are potentially useful.\n\n5. **Why might you prefer Lasso over Ridge in some cases?**\n   - When you want a simpler model by automatically excluding irrelevant features.\n\n---\n\n### **Coding-Based Questions**\n1. **How do you implement Ridge and Lasso Regression in Python using scikit-learn?**\n   ```python\n   from sklearn.linear_model import Ridge, Lasso\n   ridge = Ridge(alpha=1.0)\n   lasso = Lasso(alpha=0.1)\n   ridge.fit(X_train, y_train)\n   lasso.fit(X_train, y_train)\n   ```\n\n2. **How do you tune \\( \\lambda \\) (regularization parameter) in Ridge and Lasso Regression?**\n   - Use **GridSearchCV** or **RandomizedSearchCV** to find the optimal \\( \\lambda \\) (hyperparameter tuning).\n\n3. **How do you evaluate the performance of Ridge and Lasso models?**\n   - Use metrics like MSE, RMSE, and R-squared on test data and compare with Linear Regression.\n\n4. **What datasets are suitable for Ridge and Lasso Regression?**\n   - High-dimensional datasets for Ridge.\n   - Sparse datasets for Lasso where some features are irrelevant.","metadata":{"execution":{"iopub.status.busy":"2024-11-20T05:47:45.573244Z","iopub.execute_input":"2024-11-20T05:47:45.574271Z","iopub.status.idle":"2024-11-20T05:47:45.601278Z","shell.execute_reply.started":"2024-11-20T05:47:45.574223Z","shell.execute_reply":"2024-11-20T05:47:45.599037Z"}}}]}
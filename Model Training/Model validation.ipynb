{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When validating machine learning models, choosing the right metrics is essential to evaluate their performance effectively. The metrics you use depend on the type of problem (regression or classification) and the goals of your model. Here are some common metrics for summarizing model performance:\n",
    "\n",
    "### **1. Classification Metrics**\n",
    "#### **a. Accuracy**\n",
    "   - **Definition**: The ratio of correctly predicted instances to the total instances.\n",
    "   - **Use Case**: Good for balanced datasets where classes are roughly equal in frequency.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import accuracy_score\n",
    "     accuracy = accuracy_score(y_true, y_pred)\n",
    "     print(\"Accuracy:\", accuracy)\n",
    "     ```\n",
    "\n",
    "#### **b. Precision, Recall, F1-Score**\n",
    "   - **Precision**: Measures how many of the predicted positive instances are actually positive.\n",
    "   - **Recall (Sensitivity)**: Measures how many of the actual positive instances were correctly identified.\n",
    "   - **F1-Score**: Harmonic mean of precision and recall; useful when you want a balance between precision and recall.\n",
    "   - **Use Case**: Suitable for imbalanced datasets.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "     precision = precision_score(y_true, y_pred, average='weighted')\n",
    "     recall = recall_score(y_true, y_pred, average='weighted')\n",
    "     f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "     print(\"Precision:\", precision)\n",
    "     print(\"Recall:\", recall)\n",
    "     print(\"F1-Score:\", f1)\n",
    "     ```\n",
    "\n",
    "#### **c. Confusion Matrix**\n",
    "   - **Definition**: A matrix that shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "   - **Use Case**: Gives a detailed breakdown of classification performance.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "     cm = confusion_matrix(y_true, y_pred)\n",
    "     print(\"Confusion Matrix:\\n\", cm)\n",
    "     ```\n",
    "\n",
    "#### **d. ROC Curve and AUC (Area Under Curve)**\n",
    "   - **ROC Curve**: Graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate.\n",
    "   - **AUC**: The area under the ROC curve; higher values indicate better performance.\n",
    "   - **Use Case**: Effective for binary classification problems.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import roc_curve, auc\n",
    "     fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "     auc_value = auc(fpr, tpr)\n",
    "     print(\"AUC:\", auc_value)\n",
    "     ```\n",
    "\n",
    "### **2. Regression Metrics**\n",
    "#### **a. Mean Absolute Error (MAE)**\n",
    "   - **Definition**: The average absolute difference between actual and predicted values.\n",
    "   - **Use Case**: When all errors have equal importance.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import mean_absolute_error\n",
    "     mae = mean_absolute_error(y_true, y_pred)\n",
    "     print(\"MAE:\", mae)\n",
    "     ```\n",
    "\n",
    "#### **b. Mean Squared Error (MSE)**\n",
    "   - **Definition**: The average of the squared differences between actual and predicted values.\n",
    "   - **Use Case**: Penalizes larger errors more than smaller ones.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import mean_squared_error\n",
    "     mse = mean_squared_error(y_true, y_pred)\n",
    "     print(\"MSE:\", mse)\n",
    "     ```\n",
    "\n",
    "#### **c. Root Mean Squared Error (RMSE)**\n",
    "   - **Definition**: The square root of MSE; provides an error measure in the same units as the target variable.\n",
    "   - **Use Case**: Easier to interpret compared to MSE.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     import numpy as np\n",
    "     rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "     print(\"RMSE:\", rmse)\n",
    "     ```\n",
    "\n",
    "#### **d. R-squared (Coefficient of Determination)**\n",
    "   - **Definition**: Measures the proportion of variance in the target variable that is predictable from the features.\n",
    "   - **Range**: 0 to 1 (closer to 1 indicates a better fit).\n",
    "   - **Use Case**: Useful to understand how well the model explains the data.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.metrics import r2_score\n",
    "     r2 = r2_score(y_true, y_pred)\n",
    "     print(\"R-squared:\", r2)\n",
    "     ```\n",
    "\n",
    "### **3. Cross-Validation Scores**\n",
    "   - **Definition**: Evaluating the model's performance on multiple splits of the data to get a more reliable estimate.\n",
    "   - **Use Case**: Helps prevent overfitting by showing how the model performs on unseen data.\n",
    "   - **Code Example**:\n",
    "     ```python\n",
    "     from sklearn.model_selection import cross_val_score\n",
    "     scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n",
    "     print(\"Cross-Validation Scores:\", scores)\n",
    "     print(\"Average Score:\", scores.mean())\n",
    "     ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
